{"block_file": {"data_exporters/move_to_silver.py:data_exporter:python:move to silver": {"content": "import boto3\nimport json\nimport os\nfrom datetime import datetime\n\n@data_exporter\ndef move_to_silver(output, *args, **kwargs):\n    \"\"\"\n    Mover datos validados a Silver layer en S3\n    \"\"\"\n    if output['data'].empty:\n        print(\"No hay datos para mover a Silver\")\n        return output\n    \n    df = output['data']\n    \n    # Configuraci\u00f3n S3\n    s3 = boto3.client(\n        's3',\n        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n        region_name=os.getenv('AWS_DEFAULT_REGION', 'us-east-1'),\n        endpoint_url=os.getenv('AWS_ENDPOINT_URL')\n    )\n    \n    bucket_name = os.getenv('BRONZE_BUCKET', 'city-data-25')\n    now = datetime.utcnow()\n    \n    # Filtrar solo datos v\u00e1lidos para Silver\n    silver_data = df[df['is_valid_quality']].copy()\n    \n    if not silver_data.empty:\n        # Guardar cada registro en Silver\n        for _, row in silver_data.iterrows():\n            try:\n                silver_key = f\"silver/trips/date={now.strftime('%Y-%m-%d')}/hour={now.strftime('%H')}/{row['trip_id']}.json\"\n                \n                s3.put_object(\n                    Bucket=bucket_name,\n                    Key=silver_key,\n                    Body=json.dumps(row.to_dict(), default=str),\n                    ContentType='application/json'\n                )\n                \n            except Exception as e:\n                print(f\"Error guardando {row['trip_id']} en Silver: {e}\")\n    \n    print(f\"{len(silver_data)} registros movidos a Silver layer\")\n    \n    # Guardar reporte de calidad\n    if 'quality_stats' in output:\n        report_key = f\"reports/quality/date={now.strftime('%Y-%m-%d')}/hour={now.strftime('%H')}/quality_report.json\"\n        try:\n            s3.put_object(\n                Bucket=bucket_name,\n                Key=report_key,\n                Body=json.dumps(output['quality_stats'], default=str),\n                ContentType='application/json'\n            )\n            print(f\"\ud83d\udcc4 Reporte de calidad guardado: {report_key}\")\n        except Exception as e:\n            print(f\"Error guardando reporte: {e}\")\n    \n    return output", "file_path": "data_exporters/move_to_silver.py", "language": "python", "type": "data_exporter", "uuid": "move_to_silver"}, "data_exporters/send_alerts_if_needed.py:data_exporter:python:send alerts if needed": {"content": "@data_exporter\ndef send_alerts_if_needed(metrics, *args, **kwargs):\n    \"\"\"\n    Enviar alertas si el sistema est\u00e1 degradado o hay problemas de calidad\n    \"\"\"\n    print(\"\ud83d\udea8 Verificando alertas...\")\n    \n    alerts = []\n    health_checks = metrics.get('health_checks', {})\n    \n    # 1. Verificar estado general del sistema\n    if metrics['system_status'] != 'healthy':\n        degraded_services = metrics.get('degraded_services', [])\n        alert_msg = f\"\ud83d\udea8 SISTEMA DEGRADADO - Servicios afectados: {degraded_services}\"\n        print(alert_msg)\n        alerts.append({\n            'type': 'system_degraded',\n            'message': alert_msg,\n            'severity': 'high'\n        })\n    \n    # 2. Verificar API espec\u00edficamente\n    if 'api' in health_checks and health_checks['api']['status'] != 'healthy':\n        alert_msg = f\"\ud83c\udf10 API CA\u00cdDA - {health_checks['api'].get('error', 'No disponible')}\"\n        print(alert_msg)\n        alerts.append({\n            'type': 'api_down',\n            'message': alert_msg,\n            'severity': 'high'\n        })\n    \n    # 3. Verificar calidad de datos\n    if 'data_quality' in metrics:\n        quality = metrics['data_quality']\n        avg_score = quality.get('avg_quality_score', 100)\n        total_trips = quality.get('total_trips_last_hour', 0)\n        valid_trips = quality.get('valid_trips_last_hour', 0)\n        \n        # Alerta por calidad baja\n        if avg_score < 70:\n            alert_msg = f\"\u26a0\ufe0f  CALIDAD DE DATOS BAJA - Score promedio: {avg_score:.1f}\"\n            print(alert_msg)\n            alerts.append({\n                'type': 'low_data_quality',\n                'message': alert_msg,\n                'severity': 'medium',\n                'avg_score': avg_score\n            })\n        \n        # Alerta por alta tasa de datos inv\u00e1lidos (>20%)\n        if total_trips > 0:\n            valid_ratio = valid_trips / total_trips\n            if valid_ratio < 0.8:\n                alert_msg = f\"\u26a0\ufe0f  ALTA TASA DE DATOS INV\u00c1LIDOS - {valid_trips}/{total_trips} v\u00e1lidos ({valid_ratio:.1%})\"\n                print(alert_msg)\n                alerts.append({\n                    'type': 'high_invalid_data',\n                    'message': alert_msg,\n                    'severity': 'medium',\n                    'valid_ratio': valid_ratio\n                })\n    \n    # 4. Verificar cola SQS saturada\n    if 'sqs' in health_checks and health_checks['sqs']['status'] == 'healthy':\n        message_count = health_checks['sqs'].get('message_count', 0)\n        if message_count > 1000:\n            alert_msg = f\"\ud83d\udce8 COLA SQS SATURADA - {message_count} mensajes pendientes\"\n            print(alert_msg)\n            alerts.append({\n                'type': 'sqs_backlog',\n                'message': alert_msg,\n                'severity': 'medium',\n                'message_count': message_count\n            })\n    \n    # 5. Guardar m\u00e9tricas en S3 para dashboard\n    try:\n        s3 = boto3.client(\n            's3',\n            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n            region_name=os.getenv('AWS_DEFAULT_REGION', 'us-east-1'),\n            endpoint_url=os.getenv('AWS_ENDPOINT_URL')\n        )\n        \n        bucket_name = os.getenv('BRONZE_BUCKET', 'city-data-25')\n        now = datetime.utcnow()\n        \n        # Guardar m\u00e9tricas del sistema\n        metrics_key = f\"monitoring/system_metrics/{now.strftime('%Y-%m-%d/%H')}/metrics_{now.strftime('%H%M')}.json\"\n        \n        s3.put_object(\n            Bucket=bucket_name,\n            Key=metrics_key,\n            Body=json.dumps(metrics, default=str, indent=2),\n            ContentType='application/json'\n        )\n        \n        print(f\"M\u00e9tricas guardadas en: {metrics_key}\")\n        \n        # Guardar alertas si las hay\n        if alerts:\n            alerts_key = f\"monitoring/alerts/{now.strftime('%Y-%m-%d')}/alerts_{now.strftime('%H%M')}.json\"\n            \n            alerts_data = {\n                'timestamp': now.isoformat(),\n                'alert_count': len(alerts),\n                'alerts': alerts\n            }\n            \n            s3.put_object(\n                Bucket=bucket_name,\n                Key=alerts_key,\n                Body=json.dumps(alerts_data, default=str, indent=2),\n                ContentType='application/json'\n            )\n            \n            print(f\"Alertas guardadas en: {alerts_key}\")\n        \n    except Exception as e:\n        print(f\"Error guardando en S3: {e}\")\n    \n    # 5. Resumen final\n    if not alerts:\n        print(\"Todo funcionando correctamente - Sin alertas\")\n        summary = \"SISTEMA SALUDABLE\"\n    else:\n        print(f\"Se generaron {len(alerts)} alertas\")\n        \n        # Contar alertas por severidad\n        high_alerts = len([a for a in alerts if a.get('severity') == 'high'])\n        medium_alerts = len([a for a in alerts if a.get('severity') == 'medium'])\n        \n        summary = f\"{high_alerts} ALERTAS CR\u00cdTICAS - {medium_alerts} ADVERTENCIAS\"\n    \n    # Retornar resultado completo\n    return {\n        'timestamp': datetime.utcnow().isoformat(),\n        'system_status': metrics['system_status'],\n        'alert_count': len(alerts),\n        'alerts_generated': alerts,\n        'summary': f\"{len(alerts)} alertas generadas\",\n        'api_status': health_checks.get('api', {}).get('status', 'unknown')\n    }", "file_path": "data_exporters/send_alerts_if_needed.py", "language": "python", "type": "data_exporter", "uuid": "send_alerts_if_needed"}, "data_loaders/check_new_realtime_data.py:data_loader:python:check new realtime data": {"content": "import boto3\nimport pandas as pd\nimport json\nimport os\nfrom datetime import datetime, timedelta\n\n@data_loader\ndef check_new_realtime_data(*args, **kwargs):\n    \"\"\"\n    Buscar nuevos datos REALES en Bronze layer de los \u00faltimos 15 minutos\n    \"\"\"\n    print(\"Buscando datos NUEVOS reales en Bronze...\")\n    \n    # Configuraci\u00f3n S3\n    s3 = boto3.client(\n        's3',\n        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n        region_name=os.getenv('AWS_DEFAULT_REGION', 'us-east-1'),\n        endpoint_url=os.getenv('AWS_ENDPOINT_URL')\n    )\n    \n    bucket_name = os.getenv('BRONZE_BUCKET', 'city-data-25')\n    \n    # Calcular rango de tiempo (\u00faltimos 15 minutos)\n    now = datetime.utcnow()\n    start_time = now - timedelta(minutes=15)\n    \n    print(f\"Buscando archivos desde: {start_time} hasta: {now}\")\n    \n    try:\n        # Buscar en TODAS las carpetas de fecha/hora\n        all_objects = []\n        continuation_token = None\n        \n        # Paginaci\u00f3n para listar todos los objetos\n        while True:\n            list_params = {\n                'Bucket': bucket_name,\n                'Prefix': 'bronze/trips/',\n                'MaxKeys': 1000\n            }\n            if continuation_token:\n                list_params['ContinuationToken'] = continuation_token\n            \n            response = s3.list_objects_v2(**list_params)\n            \n            if 'Contents' in response:\n                all_objects.extend(response['Contents'])\n            \n            if not response.get('IsTruncated'):\n                break\n            continuation_token = response.get('NextContinuationToken')\n        \n        print(f\"Total de objetos en Bronze: {len(all_objects)}\")\n        \n        # Filtrar archivos de los \u00faltimos 15 minutos\n        new_files = []\n        for obj in all_objects:\n            file_time = obj['LastModified'].replace(tzinfo=None)\n            if start_time <= file_time <= now:\n                new_files.append({\n                    'key': obj['Key'],\n                    'size': obj['Size'],\n                    'last_modified': file_time\n                })\n        \n        print(f\"Archivos nuevos encontrados: {len(new_files)}\")\n        \n        # Cargar datos REALES de los archivos nuevos\n        all_data = []\n        for file_info in new_files:\n            try:\n                # Descargar archivo de S3\n                response = s3.get_object(Bucket=bucket_name, Key=file_info['key'])\n                file_content = response['Body'].read().decode('utf-8')\n                \n                # Parsear JSON (cada archivo es un trip individual)\n                trip_data = json.loads(file_content)\n                trip_data['_s3_key'] = file_info['key']\n                trip_data['_file_size'] = file_info['size']\n                trip_data['_last_modified'] = file_info['last_modified'].isoformat()\n                \n                all_data.append(trip_data)\n                \n            except Exception as e:\n                print(f\"Error cargando {file_info['key']}: {e}\")\n                continue\n        \n        # Crear DataFrame con datos reales\n        if all_data:\n            df = pd.DataFrame(all_data)\n            print(f\"Cargados {len(df)} registros REALES de {len(new_files)} archivos\")\n            \n            # Mostrar estad\u00edsticas\n            print(f\"Rango temporal: {df['ingested_at'].min()} a {df['ingested_at'].max()}\")\n            print(f\"Tipos de bicicletas: {df['bike_type'].value_counts().to_dict()}\")\n            \n        else:\n            df = pd.DataFrame()\n            print(\"No hay datos nuevos REALES para procesar\")\n        \n        return {\n            'data': df,\n            'file_count': len(new_files),\n            'new_files': [f['key'] for f in new_files],\n            'processed_at': now.isoformat(),\n            'data_source': 'real_s3_files'\n        }\n        \n    except Exception as e:\n        print(f\"Error accediendo a S3: {e}\")\n        return {\n            'data': pd.DataFrame(), \n            'file_count': 0, \n            'error': str(e),\n            'data_source': 'error'\n        }", "file_path": "data_loaders/check_new_realtime_data.py", "language": "python", "type": "data_loader", "uuid": "check_new_realtime_data"}, "data_loaders/check_system_health.py:data_loader:python:check system health": {"content": "import boto3\nimport psycopg2\nimport requests\nimport os\nfrom datetime import datetime\n\n@data_loader\ndef check_system_health(*args, **kwargs):\n    \"\"\"\n    Verificar salud de TODOS los componentes del sistema incluyendo API\n    \"\"\"\n    print(\"\ud83c\udfe5 Verificando salud del sistema...\")\n    \n    health_checks = {}\n    \n    # 1. Verificar API\n    try:\n        api_url = \"http://api:8082/health\"  # Usando nombre del servicio Docker\n        response = requests.get(api_url, timeout=5)\n        if response.status_code == 200:\n            health_data = response.json()\n            health_checks['api'] = {\n                'status': 'healthy', \n                'service': health_data.get('service', 'ingestion-api'),\n                'response_time_ms': response.elapsed.total_seconds() * 1000\n            }\n        else:\n            health_checks['api'] = {'status': 'unhealthy', 'error': f'Status code: {response.status_code}'}\n    except Exception as e:\n        health_checks['api'] = {'status': 'unhealthy', 'error': str(e)}\n    \n    # 2. Verificar S3\n    try:\n        s3 = boto3.client(\n            's3',\n            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n            region_name=os.getenv('AWS_DEFAULT_REGION', 'us-east-1'),\n            endpoint_url=os.getenv('AWS_ENDPOINT_URL')\n        )\n        bucket_name = os.getenv('BRONZE_BUCKET', 'city-data-25')\n        s3.list_objects_v2(Bucket=bucket_name, MaxKeys=1)\n        health_checks['s3'] = {'status': 'healthy', 'bucket': bucket_name}\n    except Exception as e:\n        health_checks['s3'] = {'status': 'unhealthy', 'error': str(e)}\n    \n    # 3. Verificar PostgreSQL\n    try:\n        conn = psycopg2.connect(\n            host=os.getenv('DB_HOST'),\n            database=os.getenv('DB_NAME'),\n            user=os.getenv('DB_USER'),\n            password=os.getenv('DB_PASSWORD'),\n            port=os.getenv('DB_PORT', '5432')\n        )\n        with conn.cursor() as cur:\n            cur.execute(\"SELECT COUNT(*) FROM trip_metadata\")\n            count = cur.fetchone()[0]\n        conn.close()\n        health_checks['postgresql'] = {'status': 'healthy', 'trip_count': count}\n    except Exception as e:\n        health_checks['postgresql'] = {'status': 'unhealthy', 'error': str(e)}\n    \n    # 4. Verificar SQS\n    try:\n        sqs = boto3.client(\n            'sqs',\n            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n            region_name=os.getenv('AWS_DEFAULT_REGION', 'us-east-1'),\n            endpoint_url=os.getenv('AWS_ENDPOINT_URL')\n        )\n        queue_url = os.getenv('SQS_QUEUE_URL')\n        if queue_url:\n            response = sqs.get_queue_attributes(\n                QueueUrl=queue_url, \n                AttributeNames=['ApproximateNumberOfMessages']\n            )\n            message_count = int(response['Attributes']['ApproximateNumberOfMessages'])\n            health_checks['sqs'] = {'status': 'healthy', 'message_count': message_count}\n        else:\n            health_checks['sqs'] = {'status': 'unknown', 'error': 'No SQS_QUEUE_URL'}\n    except Exception as e:\n        health_checks['sqs'] = {'status': 'unhealthy', 'error': str(e)}\n    \n    print(f\"Chequeo de salud completado - API: {health_checks.get('api', {}).get('status', 'unknown')}\")\n    return health_checks", "file_path": "data_loaders/check_system_health.py", "language": "python", "type": "data_loader", "uuid": "check_system_health"}, "data_loaders/validate_historical_data.py:data_loader:python:validate historical data": {"content": "import boto3\nimport pandas as pd\nimport requests\nfrom datetime import datetime\nimport os\nimport json\n\n@data_loader\ndef validate_historical_data(*args, **kwargs):\n    \"\"\"\n    Validar y cargar datos hist\u00f3ricos desde S3 p\u00fablico a nuestro Bronze\n    \"\"\"\n    print(\"Validando datos hist\u00f3ricos desde S3 p\u00fablico...\")\n    \n    # Configuraci\u00f3n AWS\n    aws_access_key = os.getenv('AWS_ACCESS_KEY_ID')\n    aws_secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n    aws_region = os.getenv('AWS_DEFAULT_REGION', 'us-east-1')\n    bronze_bucket = os.getenv('BRONZE_BUCKET', 'city-data-25')\n    \n    # URL base del bucket p\u00fablico de Citi Bike\n    public_bucket_url = \"https://s3.amazonaws.com/tripdata\"\n    \n    try:\n        # Cliente S3 para nuestro bucket Bronze\n        s3 = boto3.client(\n            's3',\n            aws_access_key_id=aws_access_key,\n            aws_secret_access_key=aws_secret_key,\n            region_name=aws_region,\n            endpoint_url=os.getenv('AWS_ENDPOINT_URL')\n        )\n        \n        # Listar archivos disponibles en el bucket p\u00fablico\n        session = requests.Session()\n        response = session.get(public_bucket_url)\n        \n        if response.status_code != 200:\n            print(\"No se pudo acceder al bucket S3 p\u00fablico\")\n            return {\"status\": \"error\", \"message\": \"Cannot access public S3 bucket\"}\n        \n        # Buscar archivos recientes (\u00faltimo mes como ejemplo)\n        current_year = datetime.now().year\n        current_month = datetime.now().month\n        \n        target_filename = f\"{current_year}{current_month:02d}-citibike-tripdata.csv.zip\"\n        file_url = f\"{public_bucket_url}/{target_filename}\"\n        \n        print(f\"Buscando archivo: {target_filename}\")\n        \n        # Verificar si el archivo existe\n        head_response = session.head(file_url)\n        if head_response.status_code != 200:\n            print(f\"Archivo {target_filename} no encontrado, probando mes anterior...\")\n            # Intentar mes anterior\n            current_month -= 1\n            if current_month == 0:\n                current_month = 12\n                current_year -= 1\n            target_filename = f\"{current_year}{current_month:02d}-citibike-tripdata.csv.zip\"\n            file_url = f\"{public_bucket_url}/{target_filename}\"\n        \n        print(f\"Archivo a procesar: {target_filename}\")\n        \n        # Verificar si ya existe en nuestro Bronze\n        bronze_key = f\"historical/{target_filename}\"\n        try:\n            s3.head_object(Bucket=bronze_bucket, Key=bronze_key)\n            print(f\"Archivo ya existe en Bronze: {bronze_key}\")\n            return {\n                \"status\": \"exists\", \n                \"filename\": target_filename,\n                \"bronze_key\": bronze_key\n            }\n        except:\n            print(f\"Archivo nuevo, ser\u00e1 copiado a Bronze: {target_filename}\")\n        \n        # Retornar metadata para el siguiente bloque\n        return {\n            \"status\": \"new\",\n            \"source_url\": file_url,\n            \"filename\": target_filename,\n            \"bronze_key\": bronze_key,\n            \"bronze_bucket\": bronze_bucket\n        }\n        \n    except Exception as e:\n        print(f\"Error validando datos hist\u00f3ricos: {e}\")\n        return {\"status\": \"error\", \"message\": str(e)}", "file_path": "data_loaders/validate_historical_data.py", "language": "python", "type": "data_loader", "uuid": "validate_historical_data"}, "transformers/calculate_quality_scores.py:transformer:python:calculate quality scores": {"content": "import pandas as pd\n\n@transformer\ndef calculate_quality_scores(output, *args, **kwargs):\n    \"\"\"\n    Calcular m\u00e9tricas agregadas de calidad\n    \"\"\"\n    if output['data'].empty:\n        return output\n    \n    df = output['data']\n    \n    # Calcular bands de calidad\n    excellent = len(df[df['quality_score'] >= 90])\n    good = len(df[(df['quality_score'] >= 75) & (df['quality_score'] < 90)])\n    fair = len(df[(df['quality_score'] >= 60) & (df['quality_score'] < 75)])\n    poor = len(df[df['quality_score'] < 60])\n    \n    quality_stats = {\n        'quality_bands': {\n            'EXCELLENT': excellent,\n            'GOOD': good,\n            'FAIR': fair,\n            'POOR': poor\n        },\n        'total_records': len(df),\n        'valid_records': len(df[df['is_valid_quality']]),\n        'invalid_records': len(df[~df['is_valid_quality']]),\n        'avg_score': df['quality_score'].mean(),\n        'timestamp': pd.Timestamp.now().isoformat()\n    }\n    \n    print(f\"Resumen de calidad: {quality_stats}\")\n    \n    output['quality_stats'] = quality_stats\n    return output", "file_path": "transformers/calculate_quality_scores.py", "language": "python", "type": "transformer", "uuid": "calculate_quality_scores"}, "transformers/generate_observability_metrics.py:transformer:python:generate observability metrics": {"content": "import pandas as pd\nfrom datetime import datetime\nimport os  # \u2190 ESTA L\u00cdNEA FALTABA\n\n@transformer\ndef generate_observability_metrics(health_checks, *args, **kwargs):\n    \"\"\"\n    Generar m\u00e9tricas de observabilidad del sistema\n    \"\"\"\n    print(\"Generando m\u00e9tricas de observabilidad...\")\n    \n    # Determinar estado general del sistema\n    all_healthy = all(check['status'] == 'healthy' for check in health_checks.values())\n    \n    metrics = {\n        'timestamp': datetime.utcnow().isoformat(),\n        'system_status': 'healthy' if all_healthy else 'degraded',\n        'health_checks': health_checks,\n        'degraded_services': [\n            service for service, check in health_checks.items() \n            if check['status'] != 'healthy'\n        ]\n    }\n    \n    # Agregar m\u00e9tricas de rendimiento\n    try:\n        import psycopg2\n        conn = psycopg2.connect(\n            host=os.getenv('DB_HOST'),\n            database=os.getenv('DB_NAME'),\n            user=os.getenv('DB_USER'),\n            password=os.getenv('DB_PASSWORD')\n        )\n        \n        # M\u00e9tricas de calidad de datos recientes\n        query = \"\"\"\n        SELECT \n            AVG(quality_score) as avg_quality,\n            COUNT(*) as total_trips,\n            COUNT(CASE WHEN is_valid = true THEN 1 END) as valid_trips\n        FROM trip_metadata\n        WHERE processed_at >= NOW() - INTERVAL '1 hour'\n        \"\"\"\n        \n        df = pd.read_sql(query, conn)\n        if not df.empty:\n            metrics['data_quality'] = {\n                'avg_quality_score': float(df['avg_quality'].iloc[0]) if pd.notna(df['avg_quality'].iloc[0]) else 0,\n                'total_trips_last_hour': int(df['total_trips'].iloc[0]),\n                'valid_trips_last_hour': int(df['valid_trips'].iloc[0])\n            }\n        \n        conn.close()\n    except Exception as e:\n        print(f\"Error obteniendo m\u00e9tricas de calidad: {e}\")\n    \n    # Agregar m\u00e9tricas de SQS si est\u00e1n disponibles\n    if 'sqs' in health_checks and health_checks['sqs']['status'] == 'healthy':\n        metrics['sqs_metrics'] = {\n            'message_count': health_checks['sqs'].get('message_count', 0)\n        }\n    \n    # Agregar m\u00e9tricas de PostgreSQL si est\u00e1n disponibles\n    if 'postgresql' in health_checks and health_checks['postgresql']['status'] == 'healthy':\n        metrics['database_metrics'] = {\n            'total_trips': health_checks['postgresql'].get('trip_count', 0)\n        }\n    \n    print(f\"M\u00e9tricas del sistema: {metrics['system_status']}\")\n    print(f\"Servicios degradados: {len(metrics['degraded_services'])}\")\n    \n    if 'data_quality' in metrics:\n        quality = metrics['data_quality']\n        print(f\"Calidad datos: {quality['avg_quality_score']:.1f} avg, {quality['valid_trips_last_hour']}/{quality['total_trips_last_hour']} v\u00e1lidos\")\n    \n    return metrics", "file_path": "transformers/generate_observability_metrics.py", "language": "python", "type": "transformer", "uuid": "generate_observability_metrics"}, "transformers/load_to_bronze.py:transformer:python:load to bronze": {"content": "import boto3\nimport requests\nimport os\nfrom datetime import datetime\n\n@transformer\ndef load_to_bronze(validation_result, *args, **kwargs):\n    \"\"\"\n    Cargar datos hist\u00f3ricos validados a nuestro Bronze layer\n    \"\"\"\n    print(\"Cargando datos hist\u00f3ricos a Bronze...\")\n    \n    if validation_result.get('status') != 'new':\n        print(f\"Saltando carga: {validation_result.get('status')}\")\n        return validation_result\n    \n    try:\n        # Configuraci\u00f3n AWS\n        aws_access_key = os.getenv('AWS_ACCESS_KEY_ID')\n        aws_secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n        aws_region = os.getenv('AWS_DEFAULT_REGION', 'us-east-1')\n        \n        # Cliente S3\n        s3 = boto3.client(\n            's3',\n            aws_access_key_id=aws_access_key,\n            aws_secret_access_key=aws_secret_key,\n            region_name=aws_region,\n            endpoint_url=os.getenv('AWS_ENDPOINT_URL')\n        )\n        \n        source_url = validation_result['source_url']\n        bronze_bucket = validation_result['bronze_bucket']\n        bronze_key = validation_result['bronze_key']\n        \n        print(f\"Descargando desde: {source_url}\")\n        print(f\"Guardando en: s3://{bronze_bucket}/{bronze_key}\")\n        \n        # Descargar archivo del bucket p\u00fablico\n        session = requests.Session()\n        response = session.get(source_url, stream=True)\n        response.raise_for_status()\n        \n        # Subir a nuestro S3 Bronze\n        s3.upload_fileobj(\n            response.raw,\n            bronze_bucket,\n            bronze_key,\n            ExtraArgs={\n                'ContentType': 'application/zip',\n                'Metadata': {\n                    'source': 'nyc-citibike-public',\n                    'ingested_at': datetime.utcnow().isoformat(),\n                    'original_url': source_url\n                }\n            }\n        )\n        \n        print(f\"Archivo cargado exitosamente a Bronze: {bronze_key}\")\n        \n        # Actualizar resultado\n        validation_result['loaded_at'] = datetime.utcnow().isoformat()\n        validation_result['file_size'] = len(response.content) if hasattr(response, 'content') else 'unknown'\n        \n        return validation_result\n        \n    except Exception as e:\n        print(f\"Error cargando a Bronze: {e}\")\n        validation_result['status'] = 'error'\n        validation_result['error'] = str(e)\n        return validation_result", "file_path": "transformers/load_to_bronze.py", "language": "python", "type": "transformer", "uuid": "load_to_bronze"}, "transformers/quality_checks.py:transformer:python:quality checks": {"content": "import pandas as pd\nfrom datetime import datetime\n\n@transformer\ndef comprehensive_quality_checks(output, *args, **kwargs):\n    \"\"\"\n    Aplicar validaciones de calidad seg\u00fan especificaci\u00f3n del proyecto\n    \"\"\"\n    if output['data'].empty:\n        print(\"No hay datos para validar\")\n        return output\n    \n    df = output['data'].copy()\n    print(f\"Validando {len(df)} registros...\")\n    \n    def calculate_quality_score(row):\n        base_score = 100\n        deductions = 0\n        issues = []\n        \n        # 1. SCHEMA & COMPLETENESS (40 puntos)\n        required_fields = ['trip_id', 'bike_id', 'start_time', 'end_time', \n                          'start_station_id', 'end_station_id', 'trip_duration', 'bike_type']\n        \n        for field in required_fields:\n            if field not in row or pd.isna(row[field]) or row[field] in ['', None]:\n                deductions += 10\n                issues.append(f'missing_{field}')\n        \n        # 2. DATA VALIDITY (40 puntos)\n        try:\n            start_time = pd.to_datetime(row['start_time'])\n            end_time = pd.to_datetime(row['end_time'])\n            \n            if start_time > end_time:\n                deductions += 25\n                issues.append('invalid_time_sequence')\n            \n            # Verificar duraci\u00f3n calculada vs proporcionada\n            calculated_duration = (end_time - start_time).total_seconds()\n            provided_duration = float(row['trip_duration'])\n            \n            if abs(calculated_duration - provided_duration) > 60:\n                deductions += 15\n                issues.append('duration_mismatch')\n                \n        except:\n            deductions += 25\n            issues.append('time_parsing_error')\n        \n        # Validar rider_age\n        try:\n            rider_age = float(row.get('rider_age', 0))\n            if rider_age < 16 or rider_age > 100:\n                deductions += 20\n                issues.append('invalid_age')\n        except:\n            deductions += 10\n            issues.append('invalid_age_format')\n        \n        # 3. BUSINESS LOGIC (20 puntos)\n        try:\n            trip_duration = float(row['trip_duration'])\n            if trip_duration < 60 or trip_duration > 86400:  # <1min o >24h\n                deductions += 15\n                issues.append('invalid_duration')\n            \n            # Misma estaci\u00f3n con duraci\u00f3n > 1 hora\n            if (str(row.get('start_station_id')) == str(row.get('end_station_id')) and \n                trip_duration > 3600):\n                deductions += 5\n                issues.append('same_station_long_duration')\n                \n        except:\n            deductions += 15\n            issues.append('duration_calculation_error')\n        \n        # Calcular score final\n        final_score = max(0, (base_score - deductions))\n        \n        return pd.Series({\n            'quality_score': final_score,\n            'quality_issues': issues,\n            'is_valid_quality': final_score >= 60\n        })\n    \n    # Aplicar scoring\n    quality_data = df.apply(calculate_quality_score, axis=1)\n    \n    # Combinar con datos originales\n    for col in quality_data.columns:\n        df[col] = quality_data[col]\n    \n    output['data'] = df\n    print(f\"Validaci\u00f3n completada. Score promedio: {df['quality_score'].mean():.1f}\")\n    return output", "file_path": "transformers/quality_checks.py", "language": "python", "type": "transformer", "uuid": "quality_checks"}, "pipelines/batch_historical/metadata.yaml:pipeline:yaml:batch historical/metadata": {"content": "blocks: []\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-11-19 02:09:19.479657+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: batch_historical\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: batch_historical\nvariables_dir: /home/src/mage_data/nyc_bikes\nwidgets: []\n", "file_path": "pipelines/batch_historical/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "batch_historical/metadata"}, "pipelines/batch_historical/__init__.py:pipeline:python:batch historical/  init  ": {"content": "", "file_path": "pipelines/batch_historical/__init__.py", "language": "python", "type": "pipeline", "uuid": "batch_historical/__init__"}, "pipelines/data_quality_processing/metadata.yaml:pipeline:yaml:data quality processing/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - quality_checks\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: check_new_realtime_data\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: check_new_realtime_data\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - calculate_quality_scores\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: quality_checks\n  retry_config: null\n  status: updated\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - check_new_realtime_data\n  uuid: quality_checks\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - move_to_silver\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: calculate_quality_scores\n  retry_config: null\n  status: updated\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - quality_checks\n  uuid: calculate_quality_scores\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: move_to_silver\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - calculate_quality_scores\n  uuid: move_to_silver\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-11-19 02:08:55.561395+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: data_quality_processing\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: data_quality_processing\nvariables_dir: /home/src/mage_data/nyc_bikes\nwidgets: []\n", "file_path": "pipelines/data_quality_processing/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "data_quality_processing/metadata"}, "pipelines/data_quality_processing/__init__.py:pipeline:python:data quality processing/  init  ": {"content": "", "file_path": "pipelines/data_quality_processing/__init__.py", "language": "python", "type": "pipeline", "uuid": "data_quality_processing/__init__"}, "pipelines/system_monitoring/metadata.yaml:pipeline:yaml:system monitoring/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - generate_observability_metrics\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: check_system_health\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: check_system_health\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - send_alerts_if_needed\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: generate_observability_metrics\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - check_system_health\n  uuid: generate_observability_metrics\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: send_alerts_if_needed\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - generate_observability_metrics\n  uuid: send_alerts_if_needed\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-11-19 02:09:34.329685+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: system_monitoring\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: system_monitoring\nvariables_dir: /home/src/mage_data/nyc_bikes\nwidgets: []\n", "file_path": "pipelines/system_monitoring/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "system_monitoring/metadata"}, "pipelines/system_monitoring/__init__.py:pipeline:python:system monitoring/  init  ": {"content": "", "file_path": "pipelines/system_monitoring/__init__.py", "language": "python", "type": "pipeline", "uuid": "system_monitoring/__init__"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}